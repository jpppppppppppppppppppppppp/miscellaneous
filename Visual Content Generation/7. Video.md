Why video generation is hard?

Billion level datasets with well-defined captions is hard to collect. 



Model architecture. scale up text-2-image model. We'll discuss how to scale up in the following passages.

U-Net: integrate different resolution.

DiT.

Video Diffusion Model.



Imagen Video: text prompt -> 16x40x24 -> time SR 32x40x24  -> Spatial SR 32x80x48 -> ... -> 128x1280x768

Sora: DiT for Video Generation.



Make-A-Video (similar to Imagen Video): using pseudo-3D convo layers and pseudo-3D attention layers

Tune-A-Video: Using DDIM inversion to extract each frame's noise to edit this video.

Gen-1: Train with MiDas to extract the depth map to mimic ControlNet. Video-2-Video.

Video LDM: similar to Make-A-Video. Also have GAN's idea.

Lumiere



Training-Free Adaptation

Text2Video-Zero: DDIM inversion to get noise. Using each frame's segmentation as conditions to DDPM forward process.

ControlVideo: smoother frames.

But all can't understand the content of the video. e.g., the tree grow with people's motion.



(Google) Genie: Generate Interactive Environment.

(Meta) Movie Gen: curated clip-prompt pairs using LLaMa3. 





3D Generation

Text-to-3D.

Image-to-3D.



Surface-Based Rendering Mesh. MeshGPT.

Volume-Based Rendering Nerf/3DGS. 



DreamFusion: Text-to-3D using 2D diffusion to update NeRF weights.

SV3D: using Latent Video Diffusion.



LRM.

