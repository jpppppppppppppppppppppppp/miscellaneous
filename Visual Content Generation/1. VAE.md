We want to maximize $E_{x\sim p_{data}}\log p_\theta(x)$, with $p_{\theta}(x)$ represented as $p_\theta(x)=\displaystyle\int_z p_\theta(x|z)p(z)dz$.

Two problems:

1. We can't control **true** $p(z)$
2. We need to optimize for $\theta$. But compute integrate need numbers of samplings

Idea: we introduce a controllable distribution $q(z)$.

$\log p_\theta(x)=E_{z\sim q(z)}[\log p_\theta(x|z)] - D_{KL}(q(z)\| p_\theta(z))+D_{KL}(q(z)\|p_\theta(z|x))$.

The first two terms are tractable. So this is called **Evidence Lower Bound**.
$$
\log p_\theta(x)\geq E_{z\sim q(z)}[\log p_\theta(x|z)] - D_{KL}(q(z)\| p_\theta(z))
$$
parameterize $q(z)$ by $q_\phi(z)$ and let $q_\theta(z)$ be a simple known prior $p(z)$.



Classical Autoencoder: 
$$
\min E_x[\|G(E(x))-x\|_2^2]
$$
Variational Autoencoder:
$$
\min E_x[\|G(E(x+\varepsilon))-x\|_2^2+\|E(x+\varepsilon)\|_2^2]
$$
The regularisation term helps interpolation between different modals.
