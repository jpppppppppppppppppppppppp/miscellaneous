How to model joint distribution?

solution 1: modeling by independent latents such as VAE,

- mapping independent latents to dependent. 
- strict assumption for high-dim data
- often with low-dim latents
- a good building block, but often not sufficient

- Hard to change setting because of its poor interpretability

- Poor generation quality

solution 2: modeling by conditional distributions

- decompose a joint distribution via induce a dependency graph
- dependency graph reflect prior knowledge
- But inductive biases introduce approximations:
  - shared architectures, shared weights
  - with an induced decomposition



We can't train the AR model following its inference graph. Training: Teacher forcing.



PixelRNN: generate pixels in diagonal order to support parallelization acceleration.

PixelCNN: the convolution kernel is masked so the future context is not seen. Assume that vision generation does not require long-range dependencies.



MaskGIT: random mask.

RANDSAC: random segments.



FSQ: finite scalar quantization. use $round$ rather than $\arg\min$ to get quantized token.



VAR: next-scale or next-resolution prediction.

TiTok: 2D image encodes into 1D tokens.
