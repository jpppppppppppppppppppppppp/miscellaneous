Diffusion Model

forward process: add noise to data

reverse process: learn to denoise

training objective: from hierarchical VAE to L2 loss

noise conditional network: represent distribution


$$
-\sum_{t=2}^T E_{q(x_t|x_0)}[KL(q(x_{t-1}|x_t,x_0)\| p_\theta(x_{t-1}|x_t))]
$$
learn mean, preset variance.





Energy-based Models and Score Matching

$p(x)=\displaystyle\frac{\exp(-E(x))}{Z}$, where $Z$ is a normalizing constant. So $\nabla_x \log p(x)=-\nabla_x E(x)$, which is called score function.

When the pdf is differentiable, we can compute the gradient of a probability density.

Instead of parametrizing $p$, we can parametrize the score with Fisher divergence:
$$
D_F(p_{data}(x)\| p_\theta(x))=E_{p_{data}(x)}[\frac{1}{2}\|\nabla_x\log p_{data}(x)-\nabla_x\log p_\theta (x)\|^2]
$$
with noised data $\hat{x}=x+\varepsilon$, it can be proven:
$$
D_F(q(\hat{x})\|p_\theta (\hat{x}))=E_{q(x,\hat{x})}[\frac{1}{2}\|\nabla_{\hat{x}}\log q(\hat{x}|x)-\nabla_{\hat{x}}\log p_\theta(\hat{x})\|^2]+constant
$$
If we use Gaussian noise, $\nabla_{\hat{x}}\log q(\hat{x}|x)=\displaystyle\frac{1}{\sigma^2}(x-\hat{x})=-\displaystyle\frac{\varepsilon}{\sigma^2}$.

Langevin Dynamics: we can sample $x$ from $p$ by iterating:
$$
x_t\leftarrow x_{t-1}+\frac{\sigma^2}{2}\nabla_x\log p_\theta (x_{t-1})+\sigma z_t
$$
Here $\sigma^2/2$ works like step size, and $z_t$  works for perturbation.

Without any randomness, all output will collapse to stable points. 



Challenge in low data density regions.

- improving score estimation by adding noise. Perturbed density.

Trading off data quality and estimation accuracy. Sampling using multi-scale noise sequentially with Langevin Dynamics.

Anneal down the noise level.





Connection to SDE / ODE

In finite noise level, stochastic differential equation describes the probability densities changes in the stochastic process:

$dx_t = f(x_t,t)dt+g(t)dw_t$, the first term is the deterministic drift and the second term is infinitesimal noise.

WLOG: $dx_t=\sigma(t)dw_t$. This is forward SDE.

The reverse SDE is: $dx_t=-\sigma(t)^2 \nabla_x \log p_t(x_t)dt+\sigma(t)d\hat{w}_t$. score function exists in the first term.



Predictor-Corrector sampling methods:

predictor: numerical SDE solver. corrector: score-based MCMC.



Convert the SDE to an ODE:

SDE: $dx_t=\sigma(t)dw_t$		ODE: $\displaystyle\frac{dx_t}{dt}=-\displaystyle\frac12\sigma(t)^2\nabla_x\log p_t(x_t)$



Accelerated sampling

DDIM: 

- coarsely discrete the time axis, take big steps.
- semi-linear ODE
- 10x-50x speedups

Compared to DDPM, DDIM is able to:

- Generate high-quality samples using a much fewer number of steps
- Have 'consistency' property since the generative process is deterministic
- can do meaningful interpolation in the latent variable



Parallel ODE solving. 



Distillation. student model trained to do in 1step what DDIM achieves in 2 steps. and applied recursively to drastically reduce the number of steps required.



Consistency models are designed for one-step generation:

Self-consistency: $\forall \sigma,\sigma'\in[0,\sigma_\max]: f_\theta(x_\sigma,\sigma)=f_\theta(x_{\sigma'},\sigma')$, which is enforced via learning.

Using skip connections for enforcing the boundary condition.



Conditioned Generation

Bayes' rule for score function: $\nabla_x \log p(x|y)=\nabla_x \log p(x)+\nabla_x \log p(y|x)$. = unconditioned score + classifier 

Classifier Guided Diffusion:

training a classifier is difficult.

Classifier Free Diffusion

Train both a conditional and an unconditional score model (by randomly dropping the caption during training). Combine the two models with different guidance strength.
$$
(1+w)\nabla_{x_t}\log p(y|x_t)+\nabla_{x_t}\log p(x_t)=(1+w)\nabla _{x_t}\log p(x_t|y)-w\nabla_{x_t}\log p(x_t)
$$


Stable Diffusion: Latent Classifier Free Diffusion Model. Have a switch to train conditionally or unconditionally.

SDXL: High-resolution image synthesis. Using two-stage (base + refiner) generation. Training with resolution token to support multi-scale generation. Use crop token. 



Control Net: Adding Control Conditional to t2i. Using zero convolution to learn from empty.

DiT: Using Transformer blocks to scale diffusion model.



Diffusion-GAN: training GAN with Diffusion. Gradually increase the difficulty by feeding it samples from adaptively adjusted t.

LPIPS: The unreasonable effectiveness of deep features as a perceptual metric.



Diffusion + AR: Autoregressive image generation without vector quantization. Using AR to generate next condition, and use Diffusion to generate next token.

Diffusion Forcing: Next-token prediction meets full-sequence diffusion. 

